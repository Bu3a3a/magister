{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение моделей для предсказания показателей регионов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе предпринимается попытка создания моделей, позволяющих предсказывать:\n",
    "* Контроль над ходом госзакупок (доля отмененных конкурсов в общем количестве), %;\n",
    "* Общественное обсуждение законопроектов в Интернете (да - 1, нет - 0);\n",
    "* Количество зарегистрированных организаций ТОС на 10 тысяч человек, штук.\n",
    "\n",
    "Для предсказания используются следующие индикаторы:\n",
    "* Институционные факторы:\n",
    "    1. elections - Конкурентность выборов (да - 1, нет - 0);\n",
    "    2. parliament - Уровень парламентской конкуренции, %;\n",
    "    3. executive - Уровень конкуренции при формировании исполнительной власти, %;\n",
    "    4. citizens_election - Включенность граждан в избирательный процесс, %;\n",
    "    5. citizens_org - Участие граждан в деятельности общественных организаций, %;\n",
    "* Инфраструктурные факторы:\n",
    "    1. internet_all - Количество абонентов сети интернет, тысяч;\n",
    "    2. internet_mobile - Количество абонентов мобильного интернета на 100 человек, единиц;\n",
    "    3. internet_pc - Число персональных компьютеров на 100 человек, штук;\n",
    "    4. org_access - Организации, связанные с бизнесом и использующие интернет (от общего числа обследованных организаций), %;\n",
    "    5. org_site - Организации, связанные с бизнесом и имеющие веб-сайт (от общего числа обследованных организаций), %;\n",
    "    6. org_pc - Число персональных компьютеров на 100 работников в бизнесе, штук;\n",
    "    7. edm - Системы электронного документооборота (от общего числа обследованных организаций), %;\n",
    "    8. edm_external - Автоматический обмен данными между своими и внешними информационными системами (от общего числа обследованных организаций), %;\n",
    "    9. authority_access - Организации, имеющие отношение в органам власти и использующие интернет (от общего числа обследованных организаций), %;\n",
    "    10. public_services - Доступность госуслуг, в том числе и за счет сокращения сроков предоставления (да - 1, нет - 0);\n",
    "    11. open_data - Наличие инфраструктуры открытых данных, в том числе государственных (да - 1, нет - 0);\n",
    "    12. open_election - Открытость процесса выборов (да - 1, нет - 0);\n",
    "* Ресурсные факторы:\n",
    "    1. average_edu - Доля населения со средним образованием, %;\n",
    "    2. high_edu - Доля населения с высшим образованием, %;\n",
    "    3. degree - Доля населения, имеющего ученые степени, %;\n",
    "    4. ict - Доля специалистов в области ИКТ, %;\n",
    "    5. grp - ВРП на душу населения, рублей;\n",
    "    6. income - Среднедушевые доходы населения, рублей;\n",
    "    7. po - Объем использования программного обеспечения, %;\n",
    "    8. invest - Удельный вес инвестиций в основной капитал в ВВП, %;\n",
    "    9. venture - Доступность венчурного капитала, штук;\n",
    "    10. pc - Количество персональных компьютеров на 100 человек, штук;\n",
    "    11. nt - Затраты организаций на сетевые технологии, миллионов рублей; \n",
    "    12. ict_grp - Удельный вес затрат на ИКТ в ВРП, %.\n",
    "    \n",
    "Необходимо отметить, что в работе использовался достаточно скудный набор данных, из-за чего модели могут существенно отличаться при новом, более обширном анализе. С учетом этого, большинство шагов автоматизировано, что позволяет пересчитывать параметры моделей без внесения изменений в код."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код с комментариями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем используемые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: /home/ak/.virtualenvs/magister/lib/python2.7/site-packages/pandas/hashtable.so: undefined symbol: PyFPE_jbuf not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6a603baa283c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ak/.virtualenvs/magister/lib/python2.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m                       \u001b[1;34m\"pandas from the source directory, you may need to run \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                       \u001b[1;34m\"'python setup.py build_ext --inplace' to build the C \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                       \"extensions first.\".format(module))\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: C extension: /home/ak/.virtualenvs/magister/lib/python2.7/site-packages/pandas/hashtable.so: undefined symbol: PyFPE_jbuf not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import display\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настраиваем графики для отображения в теле блокнота без скролла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем глобальные константы - предельный уровень корреляции, список непрерывных типов данных, метод измерения качества модели, списки классификаторов и регрессоров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORRELLATION_LIMIT = 0.85\n",
    "NUMERIC_TYPES = ['float16', 'float32', 'float64', 'complex64', 'complex128']\n",
    "SCORING_CLF = 'r2'\n",
    "SCORING_RGR = 'r2'\n",
    "CLASSIFIERS = [linear_model.LogisticRegression, ensemble.GradientBoostingClassifier]\n",
    "REGRESSORS = [linear_model.Ridge, ensemble.GradientBoostingRegressor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь с расшифровками имен факторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FACTOR_NAMES = {\n",
    "    'elections': u'Конкурентность выборов (да - 1, нет - 0)',\n",
    "    'parliament': u'Уровень парламентской конкуренции, %',\n",
    "    'executive': u'Уровень конкуренции при формировании исполнительной власти, %',\n",
    "    'citizens_election': u'Включенность граждан в избирательный процесс, %',\n",
    "    'citizens_org': u'Участие граждан в деятельности общественных организаций, %',\n",
    "    'internet_all': u'Количество абонентов сети интернет, тысяч',\n",
    "    'internet_mobile': u'Количество абонентов мобильного интернета на 100 человек, единиц',\n",
    "    'internet_pc': u'Число персональных компьютеров на 100 человек, штук',\n",
    "    'org_access': u'Организации, связанные с бизнесом и использующие интернет (от общего числа обследованных организаций), %',\n",
    "    'org_site': u'Организации, связанные с бизнесом и имеющие веб-сайт (от общего числа обследованных организаций), %',\n",
    "    'org_pc': u'Число персональных компьютеров на 100 работников в бизнесе, штук',\n",
    "    'edm': u'Системы электронного документооборота (от общего числа обследованных организаций), %',\n",
    "    'edm_external': u'Автоматический обмен данными между своими и внешними информационными системами (от общего числа обследованных организаций), %',\n",
    "    'authority_access': u'Организации, имеющие отношение в органам власти и использующие интернет (от общего числа обследованных организаций), %',\n",
    "    'public_services': u'Доступность госуслуг, в том числе и за счет сокращения сроков предоставления (да - 1, нет - 0)',\n",
    "    'open_data': u'Наличие инфраструктуры открытых данных, в том числе государственных (да - 1, нет - 0)',\n",
    "    'open_election': u'Открытость процесса выборов (да - 1, нет - 0)',\n",
    "    'average_edu': u'Доля населения со средним образованием, %',\n",
    "    'high_edu': u'Доля населения с высшим образованием, %',\n",
    "    'degree': u'Доля населения, имеющего ученые степени, %',\n",
    "    'ict': u'Доля специалистов в области ИКТ, %',\n",
    "    'grp': u'ВРП на душу населения, рублей',\n",
    "    'income': u'Среднедушевые доходы населения, рублей',\n",
    "    'po': u'Объем использования программного обеспечения, %',\n",
    "    'invest': u'Удельный вес инвестиций в основной капитал в ВВП, %',\n",
    "    'venture': u'Доступность венчурного капитала, штук',\n",
    "    'pc': u'Количество персональных компьютеров на 100 человек, штук',\n",
    "    'nt': u'Затраты организаций на сетевые технологии, миллионов рублей',\n",
    "    'ict_grp': u'Удельный вес затрат на ИКТ в ВРП, %',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем данные из соответствующих файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "institutional_factors = pd.read_csv('institutional_factors.csv', index_col='index')\n",
    "institutional_factors.df_name = u'Институциональные факторы'\n",
    "\n",
    "infrastructural_factors = pd.read_csv('infrastructural_factors.csv', index_col='index')\n",
    "infrastructural_factors.df_name = u'Инфраструктурные факторы'\n",
    "\n",
    "resource_factors = pd.read_csv('resource_factors.csv', index_col='index')\n",
    "resource_factors.df_name = u'Ресурсные факторы'\n",
    "\n",
    "factors = [institutional_factors, infrastructural_factors, resource_factors]\n",
    "targets = pd.read_csv('target.csv', index_col='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучим характеристики таблиц факторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in factors:\n",
    "    print '\\n' + df.df_name.upper()\n",
    "    display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые индикаторы принимают лишь одно значение на изучаемой выборке и бесполезны для построения модели. Удалим их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleared_factors = []\n",
    "for df in factors:\n",
    "    for column in df.columns:\n",
    "        if df.min()[column] == df.max()[column]:\n",
    "            name = df.df_name\n",
    "            df = df.drop(column, axis=1)\n",
    "            df.df_name = name\n",
    "    cleared_factors.append(df)\n",
    "for df in cleared_factors:\n",
    "    print '\\n' + df.df_name.upper()\n",
    "    display(df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь взглянем на корреляции факторов в каждой группе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for df in cleared_factors:\n",
    "    f, ax = plt.subplots(figsize=(15, 15))\n",
    "    sb.plt.title('\\n' + df.df_name.upper())\n",
    "    sb.corrplot(df)\n",
    "#     corr = np.corrcoef(df)\n",
    "#     mask = np.zeros_like(corr)\n",
    "#     mask[np.triu_indices_from(mask)] = True\n",
    "#     with sb.axes_style(\"white\"):\n",
    "#         ax = sb.heatmap(corr, mask=mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Некоторые факторы тесно коррелируют друг с другом (> CORRELLATION_LIMIT по модулю). Посмотрим на них поближе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlated = {df.df_name: [] for df in cleared_factors}\n",
    "for df in cleared_factors:\n",
    "    correlation = df.corr()\n",
    "    for c1 in df.columns:\n",
    "        for c2 in df.columns:\n",
    "            if abs(correlation[c1][c2]) > CORRELLATION_LIMIT and c1 != c2:\n",
    "                if [c2, c1, correlation[c1][c2]] not in correlated[df.df_name]:\n",
    "                    correlated[df.df_name].append([c1, c2, correlation[c1][c2]])\n",
    "for df_name, items in correlated.iteritems():\n",
    "    if correlated[df_name]: print '\\n' + df_name\n",
    "    for item in items:\n",
    "        print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из каждой группы удалим минимально необходимое для устранения тесных корреляций число факторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uncorrelated_factors = []\n",
    "for df in cleared_factors:\n",
    "    while correlated[df.df_name]:\n",
    "        frequencies, removed = {}, []\n",
    "        for item in correlated[df.df_name]:\n",
    "            if item[0] in frequencies and item[1] in frequencies:\n",
    "                frequencies[item[0]] += 1\n",
    "                frequencies[item[1]] += 1\n",
    "            elif item[0] in frequencies and item[1] not in frequencies:\n",
    "                frequencies[item[0]] += 1\n",
    "                frequencies[item[1]] = 1\n",
    "            elif item[0] not in frequencies and item[1] in frequencies:\n",
    "                frequencies[item[0]] = 1\n",
    "                frequencies[item[1]] += 1\n",
    "            else:\n",
    "                frequencies[item[0]] = 1\n",
    "                frequencies[item[1]] = 1\n",
    "        most_frequent = max(frequencies, key=lambda i: frequencies[i])\n",
    "        for item in list(correlated[df.df_name]):\n",
    "            if most_frequent in item:\n",
    "                correlated[df.df_name].remove(item)\n",
    "        name = df.df_name\n",
    "        df = df.drop(most_frequent, axis=1)\n",
    "        df.df_name = name\n",
    "    uncorrelated_factors.append(df)\n",
    "for df in uncorrelated_factors:\n",
    "    print '\\n' + df.df_name.upper()\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого набора по оставшимся факторам отберем наилучшие модели по качеству предсказания бинарных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_groups = targets.columns.to_series().groupby(targets.dtypes).groups\n",
    "categorial_targets = [sublist for k, v in target_groups.iteritems() if k.name not in NUMERIC_TYPES for sublist in v]\n",
    "print u'Бинарные целевые переменные: \\n'\n",
    "for ct in categorial_targets: print ct\n",
    "print u'\\n\\nНепрерывные целевые переменные: \\n'\n",
    "numeric_targets = [nt for nt in set(targets.columns) - set(categorial_targets)]\n",
    "for nt in numeric_targets: print nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = cross_validation.KFold(len(targets), n_folds=3, shuffle=True)  # используем это разбиение для всех методов\n",
    "clfs = CLASSIFIERS\n",
    "c_values, n_trees = np.logspace(-4, 1, 6), np.arange(5, 50, 6) \n",
    "best_rez_clf = {tname: {fs.df_name: {'score': -999999} for fs in uncorrelated_factors} for tname in categorial_targets}\n",
    "for target_name in categorial_targets:\n",
    "    for factors in uncorrelated_factors:\n",
    "        target = targets.loc[:, target_name]\n",
    "        for classifier in clfs:\n",
    "            for c, n in zip(c_values, n_trees):\n",
    "#                 start_time = datetime.datetime.now()\n",
    "                try:\n",
    "                    clf = classifier(C=c).fit(factors, target)\n",
    "                except TypeError:\n",
    "                    clf = classifier(n_estimators=n).fit(factors, target)\n",
    "                score_list = cross_validation.cross_val_score(clf, factors, target, scoring=SCORING_CLF, cv=kf)\n",
    "                average_val_score = sum(score_list) / float(len(score_list))\n",
    "#                 end_time = datetime.datetime.now() - start_time\n",
    "#                 hours, remainder = divmod(end_time.total_seconds() , 3600)\n",
    "#                 minutes, seconds = divmod(remainder, 60)\n",
    "#                 print '%s, %s, var = %.5f, av_score = %.5f, time = %s:%s' % \\\n",
    "#                         (factors.df_name, classifier.__name__, c, average_val_score, minutes, seconds)\n",
    "                if best_rez_clf[target_name][factors.df_name]['score'] < average_val_score:\n",
    "                    best_rez_clf[target_name][factors.df_name]['score'] = average_val_score\n",
    "                    best_rez_clf[target_name][factors.df_name]['var'] = c\n",
    "                    if classifier.__name__ == 'GradientBoostingClassifier': \n",
    "                        best_rez_clf[target_name][factors.df_name]['var'] = n\n",
    "                    best_rez_clf[target_name][factors.df_name]['clf'] = clf\n",
    "                    # best_rez_clf[target_name][factors.df_name]['coef'] = clf.coef_\n",
    "for target in best_rez_clf:\n",
    "    print target\n",
    "    for fname in best_rez_clf[target]:\n",
    "        print '\\t' + fname\n",
    "        for k, v in best_rez_clf[target][fname].iteritems():\n",
    "            print '\\t\\t' + k + ': ' + str(v)[:60]\n",
    "            \n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for target_name in categorial_targets:\n",
    "    for factors in uncorrelated_factors:\n",
    "        scfac = preprocessing.scale(factors)\n",
    "        target = targets.loc[:, target_name]\n",
    "        for classifier in [clfs[0]]:\n",
    "                c = 0.01\n",
    "#                 start_time = datetime.datetime.now()\n",
    "                try:\n",
    "                    clf = classifier(C=c).fit(scfac, target)\n",
    "                except TypeError:\n",
    "                    clf = classifier(n_estimators=n).fit(scfac, target)\n",
    "                print factors.df_name\n",
    "                print target_name    \n",
    "                print '----------------------------------------'\n",
    "                print 'Column names:'\n",
    "                print list(factors.columns)\n",
    "                print '----------------------------------------'\n",
    "                print 'Coefficients:'\n",
    "                print clf.coef_\n",
    "                print '----------------------------------------'\n",
    "#                 print 'Means:'\n",
    "#                 print scfac.mean()\n",
    "#                 print '----------------------------------------'\n",
    "#                 print 'Coef * Mean:'\n",
    "#                 print scfac.mean()*clf.coef_[0]\n",
    "#                 print '----------------------------------------'\n",
    "                print 'Full names:'\n",
    "                for col_name in list(factors.columns):\n",
    "                    print col_name, '-', FACTOR_NAMES[col_name]\n",
    "                print ''\n",
    "                print '========================================'\n",
    "                \n",
    "\n",
    "print '========================================'\n",
    "print ''\n",
    "\n",
    "for target_name in numeric_targets:\n",
    "    for factors in uncorrelated_factors:\n",
    "        target = targets.loc[:, target_name]\n",
    "        for classifier in [linear_model.LinearRegression]:\n",
    "                clf = classifier().fit(factors, target)\n",
    "                print factors.df_name\n",
    "                print target_name    \n",
    "                print '----------------------------------------'\n",
    "                print 'Column names:'\n",
    "                print list(factors.columns)\n",
    "                print '----------------------------------------'\n",
    "                print 'Coefficients:'\n",
    "                print clf.coef_\n",
    "                print '----------------------------------------'\n",
    "                print 'Means:'\n",
    "                print factors.mean()\n",
    "                print '----------------------------------------'\n",
    "                print 'Coef * Mean:'\n",
    "                print factors.mean()*clf.coef_\n",
    "                print '----------------------------------------'\n",
    "                print 'Full names:'\n",
    "                for col_name in list(factors.columns):\n",
    "                    print col_name, '-', FACTOR_NAMES[col_name]\n",
    "                print ''\n",
    "                print '========================================'\n",
    "    print '========================================'\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "for target_name in categorial_targets:\n",
    "    for factors in uncorrelated_factors:\n",
    "        target = targets.loc[:, target_name]\n",
    "        for classifier in [clfs[0]]:\n",
    "                c = 0.01\n",
    "#                 start_time = datetime.datetime.now()\n",
    "                try:\n",
    "                    clf = classifier(C=c).fit(scfac, target)\n",
    "                except TypeError:\n",
    "                    clf = classifier(n_estimators=n).fit(scfac, target)\n",
    "                print factors.df_name\n",
    "                print target_name    \n",
    "                print '----------------------------------------'\n",
    "                print 'Column names:'\n",
    "                print list(factors.columns)\n",
    "                print '----------------------------------------'\n",
    "\n",
    "                #rank all features, i.e continue the elimination until the last one\n",
    "                rfe = RFE(clf, n_features_to_select=1)\n",
    "                rfe.fit(factors,target)\n",
    "\n",
    "                print \"Features sorted by their rank:\"  \n",
    "                print sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), list(factors.columns)))\n",
    "                print 'Selected features:'\n",
    "                print rfe.n_features_\n",
    "                print ''\n",
    "                print '========================================'\n",
    "                \n",
    "\n",
    "print '========================================'\n",
    "print ''\n",
    "\n",
    "for target_name in numeric_targets:\n",
    "    for factors in uncorrelated_factors:\n",
    "        target = targets.loc[:, target_name]\n",
    "        for classifier in [linear_model.LinearRegression]:\n",
    "                clf = classifier().fit(factors, target)\n",
    "                print factors.df_name\n",
    "                print target_name    \n",
    "                print '----------------------------------------'\n",
    "                print 'Column names:'\n",
    "                print list(factors.columns)\n",
    "                print '----------------------------------------'\n",
    "\n",
    "                #rank all features, i.e continue the elimination until the last one\n",
    "                rfe = RFE(clf, n_features_to_select=1)\n",
    "                rfe.fit(factors,target)\n",
    "\n",
    "                print \"Features sorted by their rank:\"  \n",
    "                print sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), list(factors.columns)))\n",
    "                print 'Selected features:'\n",
    "                print rfe.n_features_\n",
    "                print ''\n",
    "                print '========================================'\n",
    "    print '========================================'\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь для каждого набора построим модели для предсказания непрерывных целевых переменных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = REGRESSORS\n",
    "alphas, n_trees = np.logspace(-5, -1, 5), np.arange(5, 50, 6) \n",
    "best_rez_rgr = {tname: {fs.df_name: {'score': -999999} for fs in uncorrelated_factors} for tname in numeric_targets}\n",
    "for target_name in numeric_targets:\n",
    "    for factors in uncorrelated_factors:\n",
    "        target = targets.loc[:, target_name]\n",
    "        for classifier in clfs:\n",
    "            for a, n in zip(alphas, n_trees):\n",
    "#                 start_time = datetime.datetime.now()\n",
    "                try:\n",
    "                    clf = classifier(alpha=a).fit(factors, target)\n",
    "                except TypeError:\n",
    "                    clf = classifier(n_estimators=n).fit(factors, target)\n",
    "                # r2_score = 1 - residual sum of square / total sum of squares\n",
    "                score_list = cross_validation.cross_val_score(clf, factors, target, scoring=SCORING_RGR, cv=kf)\n",
    "                average_val_score = sum(score_list) / float(len(score_list))\n",
    "#                 end_time = datetime.datetime.now() - start_time\n",
    "#                 hours, remainder = divmod(end_time.total_seconds() , 3600)\n",
    "#                 minutes, seconds = divmod(remainder, 60)\n",
    "#                 print '%s, %s, var = %.5f, av_score = %.5f, time = %s:%s' % \\\n",
    "#                         (factors.df_name, classifier.__name__, c, average_val_score, minutes, seconds)\n",
    "                if best_rez_rgr[target_name][factors.df_name]['score'] < average_val_score:\n",
    "                    best_rez_rgr[target_name][factors.df_name]['score'] = average_val_score\n",
    "                    best_rez_rgr[target_name][factors.df_name]['var'] = a\n",
    "                    if classifier.__name__ == 'GradientBoostingClassifier': \n",
    "                        best_rez_rgr[target_name][factors.df_name]['var'] = n\n",
    "                    best_rez_rgr[target_name][factors.df_name]['clf'] = clf\n",
    "                    # best_rez_rgr[target_name][factors.df_name]['coef'] = clf.coef_\n",
    "for target in best_rez_rgr:\n",
    "    print target\n",
    "    for fname in best_rez_rgr[target]:\n",
    "        print '\\t' + fname\n",
    "        for k, v in best_rez_rgr[target][fname].iteritems():\n",
    "            print '\\t\\t' + k + ': ' + str(v)[:60]\n",
    "            \n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим факторы в один набор и посмотрим на корреляцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_factors = pd.concat(uncorrelated_factors, axis=1)\n",
    "print all_factors\n",
    "f, ax = plt.subplots(figsize=(20, 20))\n",
    "sb.plt.title(u'Все факторы')\n",
    "sb.corrplot(all_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_correlated = []\n",
    "correlation = all_factors.corr()\n",
    "for c1 in all_factors.columns:\n",
    "    for c2 in all_factors.columns:\n",
    "        if abs(correlation[c1][c2]) > CORRELLATION_LIMIT and c1 != c2:\n",
    "            if [c2, c1, correlation[c1][c2]] not in all_correlated:\n",
    "                all_correlated.append([c1, c2, correlation[c1][c2]])\n",
    "for item in all_correlated:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим минимально необходимое для устранения тесных корреляций число факторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_uncorrelated, removed = [], []\n",
    "while all_correlated:\n",
    "    frequencies = {}\n",
    "    for item in all_correlated:\n",
    "        if item[0] in frequencies and item[1] in frequencies:\n",
    "            frequencies[item[0]] += 1\n",
    "            frequencies[item[1]] += 1\n",
    "        elif item[0] in frequencies and item[1] not in frequencies:\n",
    "            frequencies[item[0]] += 1\n",
    "            frequencies[item[1]] = 1\n",
    "        elif item[0] not in frequencies and item[1] in frequencies:\n",
    "            frequencies[item[0]] = 1\n",
    "            frequencies[item[1]] += 1\n",
    "        else:\n",
    "            frequencies[item[0]] = 1\n",
    "            frequencies[item[1]] = 1\n",
    "    most_frequent = max(frequencies, key=lambda i: frequencies[i] if i not in removed else 0)\n",
    "    removed.append(most_frequent)\n",
    "    for item in all_correlated:\n",
    "        if most_frequent in item:\n",
    "            all_correlated.remove(item)\n",
    "    all_factors = all_factors.drop(most_frequent, axis=1)\n",
    "display(all_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Число факторов слишком велико для имеющегося количества наблюдений. Снова разобьем данные на три части, соответствующие первоначальным наборам, понизим размерность с помощью метода главных компонент для каждого из наборов до двух."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resource_columns = [c for c in resource_factors.columns if c in all_factors.columns]\n",
    "infrastractural_columns = [c for c in infrastructural_factors.columns if c in all_factors.columns]\n",
    "institutional_columns = [c for c in institutional_factors.columns if c in all_factors.columns]\n",
    "\n",
    "res = all_factors[resource_columns]\n",
    "res_scaled = preprocessing.scale(res)\n",
    "res_pca = decomposition.PCA(n_components=2)\n",
    "resource_pca = pd.DataFrame(res_pca.fit_transform(res_scaled), \n",
    "                            index=resource_factors.index, \n",
    "                            columns=['res0', 'res1'])\n",
    "\n",
    "inf = all_factors[infrastractural_columns]\n",
    "inf_scaled = preprocessing.scale(inf)\n",
    "inf_pca = decomposition.PCA(n_components=2)\n",
    "infrastructural_pca = pd.DataFrame(inf_pca.fit_transform(inf_scaled), \n",
    "                                   index=resource_factors.index, \n",
    "                                   columns=['inf0', 'inf1'])\n",
    "\n",
    "ins = all_factors[institutional_columns]\n",
    "ins_scaled = preprocessing.scale(ins)\n",
    "ins_pca = decomposition.PCA(n_components=2)\n",
    "institutional_pca = pd.DataFrame(ins_pca.fit_transform(ins_scaled), \n",
    "                                 index=resource_factors.index,\n",
    "                                 columns=['ins0', 'ins1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вновь объединим наборы и приступим к построению моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_factors_pca = pd.concat([resource_pca, infrastructural_pca, institutional_pca], axis=1)\n",
    "all_factors_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = CLASSIFIERS\n",
    "c_values, n_trees = np.logspace(-4, 1, 6), np.arange(5, 50, 6) \n",
    "pca_rez_clf = {tname: {'score': -999999} for tname in categorial_targets}\n",
    "for tname in categorial_targets:\n",
    "    target = targets.loc[:, tname]\n",
    "    for classifier in clfs:\n",
    "        for c, n in zip(c_values, n_trees):\n",
    "#                 start_time = datetime.datetime.now()\n",
    "            try:\n",
    "                clf = classifier(C=c).fit(all_factors_pca, target)\n",
    "            except TypeError:\n",
    "                clf = classifier(n_estimators=n).fit(all_factors_pca, target)\n",
    "            score_list = cross_validation.cross_val_score(clf, all_factors_pca, target, scoring=SCORING_CLF, cv=kf)\n",
    "            average_val_score = sum(score_list) / float(len(score_list))\n",
    "#                 end_time = datetime.datetime.now() - start_time\n",
    "#                 hours, remainder = divmod(end_time.total_seconds() , 3600)\n",
    "#                 minutes, seconds = divmod(remainder, 60)\n",
    "#                 print '%s, %s, var = %.5f, av_score = %.5f, time = %s:%s' % \\\n",
    "#                         (factors.df_name, classifier.__name__, c, average_val_score, minutes, seconds)\n",
    "            if pca_rez_clf[tname]['score'] < average_val_score:\n",
    "                pca_rez_clf[tname]['score'] = average_val_score\n",
    "                pca_rez_clf[tname]['var'] = c\n",
    "                if classifier.__name__ == 'GradientBoostingClassifier': \n",
    "                    pca_rez_clf[tname]['var'] = n\n",
    "                pca_rez_clf[tname]['clf'] = clf\n",
    "                # best_rez_clf[target_name][factors.df_name]['coef'] = clf.coef_\n",
    "for target in pca_rez_clf:\n",
    "    print target\n",
    "    for k, v in pca_rez_clf[target].iteritems():\n",
    "        print '\\t' + k + ': ' + str(v)[:60]\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = REGRESSORS\n",
    "alphas, n_trees = np.logspace(-5, -1, 5), np.arange(5, 50, 6) \n",
    "pca_rez_rgr = {tname: {'score': -999999} for tname in numeric_targets}\n",
    "for target_name in numeric_targets:\n",
    "    target = targets.loc[:, target_name]\n",
    "    for classifier in clfs:\n",
    "        for a, n in zip(alphas, n_trees):\n",
    "#                 start_time = datetime.datetime.now()\n",
    "            try:\n",
    "                clf = classifier(alpha=a).fit(all_factors_pca, target)\n",
    "            except TypeError:\n",
    "                clf = classifier(n_estimators=n).fit(all_factors_pca, target)\n",
    "            # r2_score = 1 - residual sum of square / total sum of squares\n",
    "            score_list = cross_validation.cross_val_score(clf, all_factors_pca, target, scoring=SCORING_RGR, cv=kf)\n",
    "            average_val_score = sum(score_list) / float(len(score_list))\n",
    "#                 end_time = datetime.datetime.now() - start_time\n",
    "#                 hours, remainder = divmod(end_time.total_seconds() , 3600)\n",
    "#                 minutes, seconds = divmod(remainder, 60)\n",
    "#                 print '%s, %s, var = %.5f, av_score = %.5f, time = %s:%s' % \\\n",
    "#                         (factors.df_name, classifier.__name__, c, average_val_score, minutes, seconds)\n",
    "            if pca_rez_rgr[target_name]['score'] < average_val_score:\n",
    "                pca_rez_rgr[target_name]['score'] = average_val_score\n",
    "                if classifier.__name__ == 'Ridge':\n",
    "                    pca_rez_rgr[target_name]['var'] = a\n",
    "                if classifier.__name__ == 'GradientBoostingRegressor': \n",
    "                    pca_rez_rgr[target_name]['var'] = n\n",
    "                pca_rez_rgr[target_name]['clf'] = clf\n",
    "                # best_rez_rgr[target_name][factors.df_name]['coef'] = clf.coef_\n",
    "for target in pca_rez_rgr:\n",
    "    print target\n",
    "    for k, v in pca_rez_rgr[target].iteritems():\n",
    "        print '\\t' + k + ': ' + str(v)[:60]\n",
    "            \n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "dot_data = tree.export_graphviz(pca_rez_clf['public_discussion']['clf'], out_file=None,\n",
    "                                feature_names=all_factors_pca.columns,  \n",
    "                                class_names=['0','1'], \n",
    "                                filled=True, rounded=True,  \n",
    "                                special_characters=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
